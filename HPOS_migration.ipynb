{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo+UHI+V4EVvQpetHHZdol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulprasanth487/S3-STORAGE-GCS-migration/blob/main/HPOS_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiuNKjc-Lfon",
        "outputId": "aad8e655-9f64-4c3d-efd8-dc7277c78ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.1.31)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-auth google-auth-oauthlib google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "import os"
      ],
      "metadata": {
        "id": "B1i9ydNHL1OM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6sGTFeNOL84d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIST THE FILES"
      ],
      "metadata": {
        "id": "zr3GUkPNMDao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_bucket_contents_with_token(bucket_name, access_token):\n",
        "    credentials = Credentials(token=access_token)\n",
        "\n",
        "    client = storage.Client(credentials=credentials, project='qwiklabs-gcp-00-1795097857bb')\n",
        "\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    blobs = bucket.list_blobs()\n",
        "\n",
        "    for blob in blobs:\n",
        "        print(blob.name)"
      ],
      "metadata": {
        "id": "UZvJiu-dME-z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_name = \"testing_migration_123\"\n",
        "acc_token=\"xxx\""
      ],
      "metadata": {
        "id": "BdtoCpPVMKDj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_bucket_contents_with_token(b_name, acc_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZtJaNT-Mcen",
        "outputId": "3a7f3d2a-57c6-4473-9699-d3b98b1101f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WhatsApp Image 2025-03-24 at 17.35.55_6adb7a11.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyqUjm7_MeiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbvX4a4hMgLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIST FILES FROM HPOS"
      ],
      "metadata": {
        "id": "-o1YC-PfMggF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkYBlKRhMiAm",
        "outputId": "c1ce8cb4-b70a-4c43-9ff0-dd9a5225def0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.37.37-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.38.0,>=1.37.37 (from boto3)\n",
            "  Downloading botocore-1.37.37-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.37->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.37->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.37->boto3) (1.17.0)\n",
            "Downloading boto3-1.37.37-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.37-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.37.37 botocore-1.37.37 jmespath-1.0.1 s3transfer-0.11.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "def list_s3_files(bucket_name, prefix=''):\n",
        "\n",
        "    s3 = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id='xxx',\n",
        "        aws_secret_access_key='xxx',\n",
        "        region_name='us-east-1'\n",
        "    )\n",
        "\n",
        "    # Initialize variables\n",
        "    files = []\n",
        "    continuation_token = None\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            'Bucket': bucket_name,\n",
        "            'Prefix': prefix\n",
        "        }\n",
        "\n",
        "        if continuation_token:\n",
        "            params['ContinuationToken'] = continuation_token\n",
        "\n",
        "        response = s3.list_objects_v2(**params)\n",
        "\n",
        "        if 'Contents' in response:\n",
        "            for obj in response['Contents']:\n",
        "                files.append(obj['Key'])\n",
        "\n",
        "        if not response.get('IsTruncated'):\n",
        "            break\n",
        "\n",
        "        continuation_token = response.get('NextContinuationToken')\n",
        "\n",
        "    return files\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    bucket_name = \"hpos-sample-buc\"\n",
        "\n",
        "    files = list_s3_files(bucket_name)\n",
        "    for file in files:\n",
        "        print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCtrrhsBOETA",
        "outputId": "d6ef37d2-fcc2-4727-92d1-7be5ba3272ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_A.png\n",
            "data_H.png\n",
            "data_P.png\n",
            "data_R.png\n",
            "data_V.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0kV9af4NOdRq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD FILES TO GCS bucket"
      ],
      "metadata": {
        "id": "OUX8bLtMO4h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google.cloud import storage\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def migrate_s3_to_gcs(s3_bucket_name, gcs_bucket_name, access_token,\n",
        "                      s3_prefix='', gcs_prefix='', project_id='qwiklabs-gcp-00-1795097857bb'):\n",
        "    \"\"\"\n",
        "    Migrate files from an AWS S3 bucket to a Google Cloud Storage bucket.\n",
        "\n",
        "    Args:\n",
        "        s3_bucket_name (str): Source S3 bucket name\n",
        "        gcs_bucket_name (str): Destination GCS bucket name\n",
        "        access_token (str): Google Cloud access token\n",
        "        s3_prefix (str): Optional prefix/folder in S3 to filter objects\n",
        "        gcs_prefix (str): Optional prefix/folder in GCS to store objects\n",
        "        project_id (str): GCP project ID\n",
        "    \"\"\"\n",
        "    # Initialize S3 client\n",
        "    s3 = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id='xxxx',\n",
        "        aws_secret_access_key='xxxx',\n",
        "        region_name='us-east-1'\n",
        "    )\n",
        "\n",
        "    # Initialize GCS client\n",
        "    credentials = Credentials(token=access_token)\n",
        "    gcs_client = storage.Client(credentials=credentials, project=project_id)\n",
        "    gcs_bucket = gcs_client.bucket(gcs_bucket_name)\n",
        "\n",
        "    # List objects in S3 bucket\n",
        "    paginator = s3.get_paginator('list_objects_v2')\n",
        "    pages = paginator.paginate(Bucket=s3_bucket_name, Prefix=s3_prefix)\n",
        "\n",
        "    # Process each object\n",
        "    for page in pages:\n",
        "        if 'Contents' not in page:\n",
        "            print(f\"No objects found in S3 bucket '{s3_bucket_name}' with prefix '{s3_prefix}'\")\n",
        "            continue\n",
        "\n",
        "        for obj in page['Contents']:\n",
        "            s3_key = obj['Key']\n",
        "\n",
        "            # Skip folders/directories (objects that end with '/')\n",
        "            if s3_key.endswith('/'):\n",
        "                print(f\"Skipping directory: {s3_key}\")\n",
        "                continue\n",
        "            # Skip files inside directories by checking if there's a '/' in the key\n",
        "            if '/' in s3_key:\n",
        "                print(f\"Skipping file in subdirectory: {s3_key}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing file: {s3_key}\")\n",
        "\n",
        "            # Create a temp file to store the S3 object content\n",
        "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                temp_file_path = temp_file.name\n",
        "\n",
        "            try:\n",
        "                # Download from S3 to temp file\n",
        "                s3.download_file(s3_bucket_name, s3_key, temp_file_path)\n",
        "\n",
        "                # Determine GCS destination path\n",
        "                file_name = os.path.basename(s3_key)\n",
        "                if gcs_prefix:\n",
        "                    gcs_destination = f\"{gcs_prefix.rstrip('/')}/{file_name}\"\n",
        "                else:\n",
        "                    gcs_destination = file_name\n",
        "\n",
        "                # Upload to GCS\n",
        "                blob = gcs_bucket.blob(gcs_destination)\n",
        "                blob.upload_from_filename(temp_file_path)\n",
        "\n",
        "                print(f\"Migrated '{s3_key}' to '{gcs_destination}' in GCS bucket '{gcs_bucket_name}'\")\n",
        "\n",
        "            finally:\n",
        "                # Clean up temp file\n",
        "                if os.path.exists(temp_file_path):\n",
        "                    os.remove(temp_file_path)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # S3 configuration\n",
        "    s3_bucket_name = \"hpos-sample-buc\"\n",
        "    s3_prefix = \"\"  # Optional: leave as empty string for entire bucket\n",
        "\n",
        "    # GCS configuration\n",
        "    gcs_bucket_name = \"testing_migration_123\"\n",
        "    gcs_prefix = \"migrated-from-s3/\"\n",
        "    acc_token=\"xxxx\"\n",
        "    project_id = \"qwiklabs-gcp-00-1795097857bb\"\n",
        "\n",
        "    # Run migration\n",
        "    migrate_s3_to_gcs(s3_bucket_name, gcs_bucket_name, acc_token, s3_prefix, gcs_prefix, project_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j1fjZzCO6ta",
        "outputId": "31a4b8b4-584e-45df-ccf8-34dec592c59d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping directory: Images/\n",
            "Skipping file in subdirectory: Images/Screenshot 2023-10-17 010409.png\n",
            "Skipping file in subdirectory: Images/Screenshot 2023-10-17 010438.png\n",
            "Skipping file in subdirectory: Images/Screenshot 2023-10-17 010519.png\n",
            "Processing file: data_A.png\n",
            "Migrated 'data_A.png' to 'migrated-from-s3/data_A.png' in GCS bucket 'testing_migration_123'\n",
            "Processing file: data_H.png\n",
            "Migrated 'data_H.png' to 'migrated-from-s3/data_H.png' in GCS bucket 'testing_migration_123'\n",
            "Processing file: data_P.png\n",
            "Migrated 'data_P.png' to 'migrated-from-s3/data_P.png' in GCS bucket 'testing_migration_123'\n",
            "Processing file: data_R.png\n",
            "Migrated 'data_R.png' to 'migrated-from-s3/data_R.png' in GCS bucket 'testing_migration_123'\n",
            "Processing file: data_V.png\n",
            "Migrated 'data_V.png' to 'migrated-from-s3/data_V.png' in GCS bucket 'testing_migration_123'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NuYQwJ7bQJzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIGRATE DIRS"
      ],
      "metadata": {
        "id": "JSzEufriRRO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def migrate_s3_to_gcs(s3_bucket_name, gcs_bucket_name, access_token,\n",
        "                      s3_prefix='', gcs_base_prefix='', project_id='qwiklabs-gcp-00-1795097857bb'):\n",
        "\n",
        "    # Initialize S3 client\n",
        "    s3 = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id='xxxx',\n",
        "        aws_secret_access_key='xxxx',\n",
        "        region_name='us-east-1'\n",
        "    )\n",
        "\n",
        "    # Initialize GCS client\n",
        "    credentials = Credentials(token=access_token)\n",
        "    gcs_client = storage.Client(credentials=credentials, project=project_id)\n",
        "    gcs_bucket = gcs_client.bucket(gcs_bucket_name)\n",
        "\n",
        "    # List objects in S3 bucket\n",
        "    paginator = s3.get_paginator('list_objects_v2')\n",
        "    pages = paginator.paginate(Bucket=s3_bucket_name, Prefix=s3_prefix)\n",
        "\n",
        "    # Track directories we've seen to create them in GCS\n",
        "    processed_dirs = set()\n",
        "\n",
        "    # Process each object\n",
        "    for page in pages:\n",
        "        if 'Contents' not in page:\n",
        "            print(f\"No objects found in S3 bucket '{s3_bucket_name}' with prefix '{s3_prefix}'\")\n",
        "            continue\n",
        "\n",
        "        for obj in page['Contents']:\n",
        "            s3_key = obj['Key']\n",
        "\n",
        "            # Skip folder placeholder objects (objects that end with '/')\n",
        "            if s3_key.endswith('/'):\n",
        "                print(f\"Skipping directory placeholder: {s3_key}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing file: {s3_key}\")\n",
        "\n",
        "            # Create a temp file to store the S3 object content\n",
        "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                temp_file_path = temp_file.name\n",
        "\n",
        "            try:\n",
        "                # Download from S3 to temp file\n",
        "                s3.download_file(s3_bucket_name, s3_key, temp_file_path)\n",
        "\n",
        "                # Preserve directory structure in GCS\n",
        "                if gcs_base_prefix:\n",
        "                    gcs_destination = f\"{gcs_base_prefix.rstrip('/')}/{s3_key}\"\n",
        "                else:\n",
        "                    gcs_destination = s3_key\n",
        "\n",
        "                # Create \"directory\" structure in GCS (not strictly necessary as GCS uses flat namespace,\n",
        "                # but helps with organization and improves console browsing experience)\n",
        "                dir_path = os.path.dirname(gcs_destination)\n",
        "                if dir_path and dir_path not in processed_dirs:\n",
        "                    # Create an empty directory marker object if needed\n",
        "                    if dir_path:\n",
        "                        dir_blob = gcs_bucket.blob(f\"{dir_path}/\")\n",
        "                        dir_blob.upload_from_string('')\n",
        "                        processed_dirs.add(dir_path)\n",
        "\n",
        "                # Upload to GCS\n",
        "                blob = gcs_bucket.blob(gcs_destination)\n",
        "                blob.upload_from_filename(temp_file_path)\n",
        "\n",
        "                print(f\"Migrated '{s3_key}' to '{gcs_destination}' in GCS bucket '{gcs_bucket_name}'\")\n",
        "\n",
        "            finally:\n",
        "                # Clean up temp file\n",
        "                if os.path.exists(temp_file_path):\n",
        "                    os.remove(temp_file_path)\n"
      ],
      "metadata": {
        "id": "AxW3Q2sVRTHG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_s3_directories(s3_bucket_name, prefix=''):\n",
        "\n",
        "    s3 = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id='xxxx',\n",
        "        aws_secret_access_key='xxxx',\n",
        "        region_name='us-east-1'\n",
        "    )\n",
        "\n",
        "    # Use the list_objects_v2 with delimiter to get directories\n",
        "    paginator = s3.get_paginator('list_objects_v2')\n",
        "    pages = paginator.paginate(Bucket=s3_bucket_name, Prefix=prefix, Delimiter='/')\n",
        "\n",
        "    directories = []\n",
        "\n",
        "    for page in pages:\n",
        "        # Add common prefixes (directories)\n",
        "        if 'CommonPrefixes' in page:\n",
        "            for common_prefix in page['CommonPrefixes']:\n",
        "                directories.append(common_prefix['Prefix'])\n",
        "\n",
        "    return directories\n"
      ],
      "metadata": {
        "id": "SFhBw_nYRhJE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def migrate_s3_directories_to_gcs(s3_bucket_name, gcs_bucket_name, access_token, project_id='qwiklabs-gcp-01-61d132ccd3b0'):\n",
        "\n",
        "    # First, list top-level directories\n",
        "    top_dirs = list_s3_directories(s3_bucket_name)\n",
        "\n",
        "    # If no directories, migrate the root\n",
        "    if not top_dirs:\n",
        "        print(\"No directories found. Migrating files from root...\")\n",
        "        migrate_s3_to_gcs(s3_bucket_name, gcs_bucket_name, access_token,\n",
        "                         s3_prefix='', gcs_base_prefix='', project_id=project_id)\n",
        "        return\n",
        "\n",
        "    # Process each directory\n",
        "    for directory in top_dirs:\n",
        "        print(f\"Processing directory: {directory}\")\n",
        "\n",
        "        # Migrate this directory\n",
        "        migrate_s3_to_gcs(\n",
        "            s3_bucket_name=s3_bucket_name,\n",
        "            gcs_bucket_name=gcs_bucket_name,\n",
        "            access_token=access_token,\n",
        "            s3_prefix=directory,\n",
        "            gcs_base_prefix='',  # Keep the original structure\n",
        "            project_id=project_id\n",
        "        )\n",
        "\n",
        "        # Recursively look for subdirectories and migrate them\n",
        "        subdirs = list_s3_directories(s3_bucket_name, directory)\n",
        "        for subdir in subdirs:\n",
        "            print(f\"Processing subdirectory: {subdir}\")\n",
        "            migrate_s3_to_gcs(\n",
        "                s3_bucket_name=s3_bucket_name,\n",
        "                gcs_bucket_name=gcs_bucket_name,\n",
        "                access_token=access_token,\n",
        "                s3_prefix=subdir,\n",
        "                gcs_base_prefix='',  # Keep the original structure\n",
        "                project_id=project_id\n",
        "            )\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "5UqHWVfpRmRJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # S3 configuration\n",
        "    s3_bucket_name = \"hpos-sample-buc\"\n",
        "\n",
        "    # GCS configuration\n",
        "    gcs_bucket_name = \"testing_migration_123\"\n",
        "    access_token = \"xxxxx\"\n",
        "    project_id = \"qwiklabs-gcp-00-1795097857bb\"\n",
        "\n",
        "    # Option 1: Run migration on entire bucket, preserving directory structure\n",
        "    migrate_s3_directories_to_gcs(s3_bucket_name, gcs_bucket_name, access_token, project_id)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bTuXJErRqI0",
        "outputId": "67b1be0b-7495-4a68-a270-eced11b07718"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing directory: Images/\n",
            "Skipping directory placeholder: Images/\n",
            "Processing file: Images/Screenshot 2023-10-17 010409.png\n",
            "Migrated 'Images/Screenshot 2023-10-17 010409.png' to 'Images/Screenshot 2023-10-17 010409.png' in GCS bucket 'testing_migration_123'\n",
            "Processing file: Images/Screenshot 2023-10-17 010438.png\n",
            "Migrated 'Images/Screenshot 2023-10-17 010438.png' to 'Images/Screenshot 2023-10-17 010438.png' in GCS bucket 'testing_migration_123'\n",
            "Processing file: Images/Screenshot 2023-10-17 010519.png\n",
            "Migrated 'Images/Screenshot 2023-10-17 010519.png' to 'Images/Screenshot 2023-10-17 010519.png' in GCS bucket 'testing_migration_123'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQNXjFngS1_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CxCZG4MBTHZA"
      }
    }
  ]
}